client = speech.speechClient()


# -----------------------------------------
(google)
config = speech.types.RecognitionConfig()
    encoding
    sample_rate_hertz
    language_code
    max_alternatives
    enable_word_time_offsets

(cb)
    encoding
    sample_rate_hertz
    language_code


# -----------------------------------------

(google)
streaming_config = StreamingRecognitionConfig()
    RecognitionConfig object
    interm_results


# -----------------------------------------

(cb)
StreamingRequest class

first time, returns StreamingRecognizeRequest
    with just streaming_config = self.config


subsequently, returns StreamingRecognizeRequest
    with audio_content = data


(google)
    return StreamingRecognizeRequest
        with audio_content = data



# -----------------------------------------
streaming_recognize
(google)
    takes streaming_config
    takes an iterator of StreamingRecognizeRequest(audio)


# Audio notes

Google only allows "1 minute" of continous audio to be streamed for recognition


Sample bit depth = 16 bit
-- implies each "sample" is "2 bytes" long

"Chunk"
Chunks are "buffers", which hold a certain number of samples
Chunks are used instead of continuous amount of audio because of the need for extra processing power
We choose a custom chunk size.

In the microphone code:
Sample bit depth = 16 bit
Sampling rate = 16000 samples per second
Chunk size = 1600 "samples"

Therefore,
    we get 16000 samples every second from microphone
    == 10 samples every second from microphone (since 1 chunk = 1600 samples)

Since Google's limitation is 1 minute of audio
    Once we send 1 minute in a continuous stream, we have to restart the stream
    1 minute = 60 seconds
    We get 10 samples every second from microphone
    Therefore, once we have "sent" Google 600 samples, we need to restart the stream

In WebAudio
Sampling Rate = 48000 samples per second (hard coded value)
Chunk size = Needs to be a power of 2





==============================
javascript API


# Create a context
# https://developer.mozilla.org/en-US/docs/Web/API/AudioContext
var audioContext = new AudioContext();


# The input is a Media Stream created from this context
# https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/createMediaStreamSource
var audioInput = context.createMediaStreamSource(stream);

# in the microphone example stream comes from user media as below
# https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia

stream = navigator.mediaDevices.getUserMedia({"audio": true, "video": false}).

# with webex, the stream comes from remote media (Webex SDK)
stream = call.remoteMediaStream

# create a recorder with a buffer (chunk size) , # of channels
# Note: for audio we only need a single channel
# https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/createScriptProcessor
var recorder = context.createScriptProcessor(this.bufferSize, 1, 1);

# Get data in the 'onaudioprocess' event
var floatSamples = e.inputBuffer.getChannelData(0);

# Note: The samples are floating point values in the range [-1, 1]
# 


==============================

